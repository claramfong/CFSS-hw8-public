---
title: "Extracting Data from NYT Developers Network"
author: "Clara Fong"
date: "`r lubridate::today()`"
output: 
  github_document:
    toc: yes
---

## Summary of Data

I wanted to look at New York Times articles related to migrants, broadly speaking. I was interested to 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = TRUE)

# Load Packages
library(tidyverse)
library(stringr)
library(httr)
library(jsonlite)
library(lubridate)
library(ggplot2)
library(tidyr)

theme_set(theme_minimal())
```

```{r a note on rtimes package}

# didn't work to use intall.packages("rtimes") because didn't work with current r version
#so instead downloaded development version from github
#install.packages("devtools")
#devtools::install_github("ropengov/rtimes")
library(rtimes)
```

```{r single request, include = FALSE}

# Create an Rprofile page to store API keys using:
#file.edit(here::here(".Rprofile"))

# Enter your NYT API key (available here: https://developer.nytimes.com/)
  #it should look like: options(nyt_key = "YOURKEYHERE")

# Set relevant parameters for GET request
key <- getOption("nyt_key") 

base.url <- "https://api.nytimes.com/svc/search/v2/articlesearch.json"
search_term <- "migrant"
filter <- 'news_desk:("Foreign")'
begin <- '20150101'
end <- '20201231'

# Putting together GET request
articles <- GET(base.url, query = list(`q` = search_term,
                                       `fq` = filter,
                                       `begin_date` = begin,
                                       `end_date` = end,
                                       `api-key` = key))

# Checking to see if URL created properly
  #articles$url <- commenting it out because it has my api key in the output

# Convert JSON lists into single text line
response <- content(articles, "text")

# Convert this text to df
response_df <- fromJSON(response, simplifyDataFrame = TRUE, flatten = TRUE) # code borrowed from: https://plsc-31101.github.io/course/collecting-data-from-the-web.html#writing-api-queries
str(response_df, max.level = 2) #we can see that we have a list of three, with our df stored under `docs`

# Extract hits, see that 2750 articles match my parameters
response_df$response$meta

# Want to pull out our df from the `docs` object
names(response_df$response$docs) # these are our column names for each 10 articles

# Store df in an object
docs <- response_df$response$docs
```

```{r request function, include = FALSE}

# Do what we did above, but try to pull all articles since the above measure can only pull ten at a time

# Create function for specified parameters above
nyt_api <- function(page){
  base.url = "https://api.nytimes.com/svc/search/v2/articlesearch.json"
    # Send GET request
    articles <- GET(base.url, query = list(`q` = search_term,
                                       `fq` = filter,
                                       `begin_date` = begin,
                                       `end_date` = end,
                                       `api-key` = key,
                                       `page` = page)) #using same GET request, only difference is adding page
    
    # Parse response to JSON
    response <- content(articles, "text")  
    response_df <- fromJSON(response, simplifyDataFrame = TRUE, flatten = TRUE) #same as above
    
    message(glue::glue("Scraping page: ", as.character(page))) # print message to track progress
    
    return(response_df$response$docs) # returns article info stored as df
}

# Test the function on next set of pages
docs_1 <- nyt_api(page = 2) # yay it works!
```

```{r building df, include=FALSE, cache = FALSE}

# Extract total hits
hits = response_df$response$meta$hits

# Store number of pages 
pages = ceiling(hits/10)

# Modify function to slow down scraping
nyt_api_slow <- slowly(f = nyt_api, rate = rate_delay(1))

# Create new df with all articles that match hit using iterative function
articles_df <- map_dfr(.x = (1:pages), ~nyt_api_slow(page = .))
```


```{r data cleaning}
# Main df
cleaned_articles_df <- articles_df %>% 
  select(snippet,
         lead_paragraph,
         pub_date,
         headline.main,
         subsection_name) %>% 
  filter(subsection_name != "What in the World",
         subsection_name != "Politics",
         subsection_name == recode(subsection_name, Canada = "Americas"))

# Also want to look at the keywords list and make it a df on its own
keywords_df <- articles_df %>% 
  unnest_longer(col = keywords) %>% 
  select(headline.main, keywords)

# keywords saved as a matrix within df, so convert all back to df
keywords_df <- do.call(data.frame, keywords_df) %>% 
  #https://stackoverflow.com/questions/30896605/dataframe-within-dataframe
  pivot_wider(names_from = keywords.name, values_from = keywords.value)

test <- articles_df %>% 
  unnest_longer(col = keywords)

test1 <- test[,"keywords"]

pivot_longer(cols = c(`1960`:`2019`), names_to = "year", values_to = "values") %>%
    pivot_wider(id_cols = -indict_name,
                names_from = indict_code, 
                values_from = values)


```

```{r analysis1}

#Distribution of Migration-Related News by Continental Region
cleaned_articles_df %>% 
  ggplot() +
  geom_bar(mapping = aes(x = subsection_name, fill = subsection_name)) +
  scale_fill_brewer(palette = "Dark2")

```

This simple data visualization shows us the frequency of events relating to migrants and migration reported by the New York Times int he past five years. Unsurprisingly, the highest count is in Europe, likely resulting to the migrant crisis during the Arab Spring movement, but it is interesting to note that Americas and Asia Pacific have relatively similar frequency, so it might be worth looking deeper into what are the kinds of stories being talked about.


```{r analysis2}
# Frequency of Migrant Articles over time

```

